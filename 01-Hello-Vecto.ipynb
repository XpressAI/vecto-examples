{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9fea162",
   "metadata": {},
   "source": [
    "# 01 Hello Vecto!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787f491",
   "metadata": {},
   "source": [
    "This is a notebook to help you get started with Vecto. You will learn how to set up Vector Space, build and experiment with a basic Vecto application. This guide will walk you through the building process step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de85083f",
   "metadata": {},
   "source": [
    "## Set Up Vecto Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy tqdm requests pillow ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af32b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from ipywidgets import interact_manual, IntSlider, FileUpload\n",
    "import pathlib\n",
    "from IPython.display import Image, display\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87862c5",
   "metadata": {},
   "source": [
    "## Make a Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a91fa",
   "metadata": {},
   "source": [
    "1. Access the Vecto login page at <[Vecto Login](https://app.vecto.ai/login.xhtml)>, insert your *Username* and *Password* and click Sign In. \n",
    "   \n",
    "  2. From the admin page sidebar, select the **Dashboard** tab and click on *Create new vector space*. Fill in the Vector Space name; in this case, we will call it `Hello_world`. You will then be able to choose a `vectorization model`. As we are going to work with both images and text, choose the [CLIP](https://github.com/openai/CLIP) model. Finally, click the `Create Vector Space` button. You can view your Vector Space details by clicking on the Vector Space name in the Vector Spaces list. Take note on your Vector Space ID to use in a later step.  \n",
    "    \n",
    "  3. In order to access the vector space, we will need to create a Vector Space authentication token. Click on the **Tokens** tab in the sidebar, set the token name to `Hello_world_token`, and then select the Vector Space `Hello_world` that we created earlier, click on `Create User Token`. Click on your token name in the list to view it. This token will be used to authenticate access to Vecto servers. Copy the token to use in the next step. Here we go! Now you have your first Vector Space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43286d54",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://docs.vecto.ai/img/docs/user-guide/Hello_world/login_vecto.gif\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7afc12a",
   "metadata": {},
   "source": [
    "## Add and Ingest Data into Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a096f8",
   "metadata": {},
   "source": [
    "To start, let's initialize the Vecto API end-point and pass our `Hello_world` Vector Space ID and authentication token. Insert the values for the `token` and `vecto_space_id`, then run the cell:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba002a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecto_base_url =\"https://api.vecto.ai/api/v0\"\n",
    "token = \"\"\n",
    "vector_space_id = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f3c93",
   "metadata": {},
   "source": [
    "Please note that the Vector Space ID and token are unique for every Vector Space. You refer to the previous step if you can not find your `Hello_world` Vector Space ID or token. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b87f54",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f151289",
   "metadata": {},
   "source": [
    "In this *Hello Vecto!* guide, we are using [LFW - People (Face Recognition) dataset from Kaggle](https://www.kaggle.com/datasets/atulanandjha/lfwpeople) which contains 13,000 `.jpg` images of different people faces. You may manually download the dataset and place it in the working directory, or use Kaggle API to download it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da92685",
   "metadata": {},
   "source": [
    "If you would like to use Kaggle, you may follow the [following steps](https://www.kaggle.com/general/74235):\n",
    "1. Go to your Kaggle account, Scroll to API section and Click Expire API Token to remove previous tokens if you've created one before.\n",
    "2. Click on Create New API Token - It will download kaggle.json file on your machine.\n",
    "3. Place kaggle.json in the current Jupyter working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e2fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run these two cells if you are using google colab\n",
    "# from google.colab import files\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edda4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir .kaggle\n",
    "# ! cp kaggle.json .kaggle/\n",
    "# ! chmod 600 .kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a380d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! kaggle datasets download \"atulanandjha/lfwpeople\"\n",
    "! unzip flipkart-products.zip -d .\n",
    "! tar -xf lfw-funneled.tgz -C ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b894c",
   "metadata": {},
   "source": [
    "\n",
    "Now your working directory should look like this:\n",
    "```\n",
    "|__simple_vecto_app\n",
    "    |__requirements.txt\n",
    "    |__vecto_application.ipynb\n",
    "    |__lfw_funneled\n",
    "        |__name1\n",
    "            |__name1_0001\n",
    "            |__name1_0002\n",
    "            ...\n",
    "        |__name2\n",
    "            |__name2_0001\n",
    "            |__name2_0002\n",
    "            ...\n",
    "```\n",
    "Now, let's set the path to our images in the notebook. First, let's find our base directory path and join it to our dataset folder; we use `list(dataset_path.glob('**/*.jpg'))` to collect all the images path in the dataset into a Python list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802c41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path().absolute()\n",
    "dataset_path = base_dir.joinpath('lfw-funneled')\n",
    "dataset_images = list(dataset_path.glob('**/*.jpg'))\n",
    "print(dataset_images[:5]) # print path to the first 5 image paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19250d",
   "metadata": {},
   "source": [
    "### Ingest the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b55c6d",
   "metadata": {},
   "source": [
    "To ingest the images in the `dataset_images` list into our `Hello_world` Vector Space, we will need a few helper functions to split the images ingesting process into batches. Let's add the two functions `ingest_image_batch` and `ingest_all_images` to our `vecto_application` notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d8268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_image_batch(batch_path_list):\n",
    "    data = {'vector_space_id': vector_space_id, 'data': [], 'modality': 'IMAGE'}\n",
    "    files = []\n",
    "    for path in batch_path_list:\n",
    "        relative = \"%s/%s\" % (path.parent.name, path.name)\n",
    "        data['data'].append(json.dumps(relative))\n",
    "        files.append(open(path, 'rb'))\n",
    "    \n",
    "    result = requests.post(\"%s/index\" % vecto_base_url,\n",
    "                               data=data,\n",
    "                               files=[('input', ('_', f, '_')) for f in files],\n",
    "                               headers={\"Authorization\": \"Bearer %s\" %token})    \n",
    "    \n",
    "    if not result.ok:\n",
    "        print(result.status_code)\n",
    "        \n",
    "    for f in files:\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a64b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_all_images(path_list, batch_size=64):\n",
    "    batch_count = math.ceil(len(path_list) / batch_size)\n",
    "    batches = [path_list[i * batch_size: (i + 1) * batch_size] for i in range(batch_count)]\n",
    "    for batch in tqdm(batches):\n",
    "        ingest_image_batch(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef5d6e",
   "metadata": {},
   "source": [
    "The batch size determines the number of images ingested in each batch. Here, we set the batch size to `64` to speed up the initial ingest process. The batch size could be set to any other integer value as low as `1`, as it depends on the dataset type and size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_all_images(path_list=dataset_images, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a64a511",
   "metadata": {},
   "source": [
    "You will need to wait for the vectorization process to finish before moving on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464d1ca",
   "metadata": {},
   "source": [
    "## Vector Search in Vector Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09be3d1",
   "metadata": {},
   "source": [
    "After the dataset ingesting finishes, we can perform multiple search queries on the unique `Hello_World` Vector Space. In this case, the queries could be images from within the dataset or external. Also, we can search for similarities for text queries as well, even though our Vector Space consists of images only.\n",
    "\n",
    "To search within the Vector Space, we need to ingest the query into a vector and search for similar data to the query vector against the whole Vector Space, then display the images with the highest similarity. For that, we will use a few helper functions to handle the mentioned processes. Let's add these four functions `display_results`, `lookup`, `text_query`, and`image_query` to our `vecto_application` notebook :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b746b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_results(results):\n",
    "    output = []\n",
    "    for result in results:\n",
    "        output.append(\"Similarity: %s\" % result['similarity'])\n",
    "        output.append(Image(dataset_path.joinpath(result['data'])))\n",
    "    display(*output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926cb440",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(f, modality, top_k):\n",
    "    result = requests.post(\"%s/lookup\" % vecto_base_url,\n",
    "                           data={'vector_space_id': vector_space_id, 'modality': modality, 'top_k': top_k},\n",
    "                           files={'query': f},\n",
    "                           headers={\"Authorization\":\"Bearer %s\" % token})\n",
    "\n",
    "    results = result.json()['results']\n",
    "    display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_query(query, top_k=10):\n",
    "    f = io.StringIO(query)\n",
    "    lookup(f, 'TEXT', top_k)\n",
    "\n",
    "def image_query(query, top_k=10):\n",
    "    f = io.BytesIO(query[0]['content'])\n",
    "    lookup(f, 'IMAGE', top_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8506a3",
   "metadata": {},
   "source": [
    "### Search using in-dataset query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc788e2",
   "metadata": {},
   "source": [
    "Let's pick an image from the dataset as our search query. Our goal is to find similar items \"people\" for that image within our `Hello_world` Vector Space. Here, we will pick this image `Aaron_Eckhart_0001.jpg`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2127793",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://docs.vecto.ai/img/docs/user-guide/Hello_world/Aaron_Eckhart_0001.jpg\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1198ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(image_query, query=FileUpload(multiple=False), top_k=IntSlider(min=1, max=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babe1e97",
   "metadata": {},
   "source": [
    "After we add and run the above line of code to our `vecto_application` notebook, you will see the following widget, upload the query image available at `lfw_funneled/Aaron_Eckhart/Aaron_Eckhart_0001.jpg`, adjust the `top_k` bar to limit the number of returning top similar items and click the `Run Interact` button to start the vector search:      \n",
    "\n",
    "The returned images of the query vector search show the image with the highest similarity is the image itself **Similarity=1.0**, and that is because our query was an image available within the Vector Space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33327ee2",
   "metadata": {},
   "source": [
    "### Search using out-of-dataset query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73530dc1",
   "metadata": {},
   "source": [
    "To check if our vector space is robust to external data, we will upload an image from outside the dataset as our query image. Similarly, you must follow the same steps in [search using in-dataset query](#Search-using-in-dataset-query). Here, we download and use this [out-of-dataset image](/img/docs/user-guide/Hello_world/out_dataset.jpg) we downloaded from [Pexels](https://www.pexels.com/photo/closeup-photo-of-woman-with-brown-coat-and-gray-top-733872/) website. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf26da",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://docs.vecto.ai/img/docs/user-guide/Hello_world/out_dataset.jpg\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0eb7cd",
   "metadata": {},
   "source": [
    "After we add and run the above line of code to our `vecto_application` notebook, the widget appears, upload the out-of-dataset image as a search query, choose a `top_k` value and click on the `Run Interact` button to start the vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c14b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(image_query, query=FileUpload(multiple=False), top_k=IntSlider(min=1, max=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5520307",
   "metadata": {},
   "source": [
    "The returned images of the query vector search are for different women with relatively similar features to our out-of-dataset image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0894cb",
   "metadata": {},
   "source": [
    "### Search using text query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a8fe",
   "metadata": {},
   "source": [
    "Finding similar data in the `Hello_world` Vector Space based on a text query is achievable too. All that needs to be done is to pass the text to the widget then Vecto will handle the text ingest and query vector search. \n",
    "\n",
    "For text query, after we add and run the above line of code to our `vecto_application` notebook, you are expected to see the following widget instead, type *Woman* as a text query into the text-box, adjust the `top_k` bar to limit the number of the returning similar items and click the `Run Interact` button to start the vector search.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f0093",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(text_query, query=\"Women\", top_k=IntSlider(min=1, max=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adff4a5",
   "metadata": {},
   "source": [
    "The returned images of the text query vector search are for different women. Now try other text queries and analyze the vector search output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a3e2c8",
   "metadata": {},
   "source": [
    "## Create and Apply Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f214c2e2",
   "metadata": {},
   "source": [
    "Analogy completion via vector arithmetic has become a common means of demonstrating the compositionality of embeddings. Taking `Men is to King as Women is to Queen` as an analogy, we could use the vector difference *King vector - Men vector* as an analogy vector to modify the vector search output for *Women* query from returning images of Women in a straight forward query search as shown [above](#search-using-text-query) to returning images of Queens instead. The overall arithmetic equation that governs such an analogy can be represented as follows:\n",
    "\n",
    "<div align=\"center\"><br>\n",
    "Let <strong>Men - King = Women - Queen</strong><br>\n",
    "Therefore, <strong>Men - King + Women = Queen</strong>\n",
    "</div>\n",
    "\n",
    "To construct an analogy you need 3 components:\n",
    "1. The *start* of the analogy, in this example is **Men** \n",
    "2. The *end* of the analogy, in this example is **King**\n",
    "3. The *query* to apply the analogy on, in this example is **Women**\n",
    "\n",
    "\n",
    "To apply the analogy to a vector search query, we need to determine the analogy *start* and *end* vector difference, then add it to the *query* vector before finding similarity within the vector space. For that, let's add this three functions `analogy`,`text_analogy` and `image_analogy` to our `vecto_application` notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(query, start, end, modality, top_k):\n",
    "\n",
    "    result = requests.post(\"%s/analogy\" % vecto_base_url,\n",
    "                           data={'vector_space_id': vector_space_id, 'modality': modality, 'top_k': top_k},\n",
    "                           files={'query': query, 'from': start, 'to': end},\n",
    "                           headers={\"Authorization\":\"Bearer %s\" %token})\n",
    "    \n",
    "    print(result)\n",
    "    results = result.json()['results']\n",
    "    display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eeb506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_analogy(query, start, end, top_k=10):\n",
    "    analogy(io.StringIO(query), io.StringIO(start), io.StringIO(end), 'TEXT', top_k)\n",
    "\n",
    "def image_analogy(query, start, end, top_k=10):\n",
    "    analogy(\n",
    "        io.BytesIO(list(query.values())[0]['content']),\n",
    "        io.BytesIO(list(start.values())[0]['content']),\n",
    "        io.BytesIO(list(end.values())[0]['content']),\n",
    "        'IMAGE',\n",
    "        top_k\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2a299",
   "metadata": {},
   "source": [
    "### Dynamic Analogy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b12ea17",
   "metadata": {},
   "source": [
    "Dynamic analogy allows experimenting with different analogy's *start* and *end* to create a difference vector that generalizes to multiple queries. All that needs to be done is to set text to the analogy's *start*, *end* and *query*.\n",
    "\n",
    "For a dynamic analogy, after we add and run the above line of code to our `vecto_application` notebook, you are expected to see the following widgets, type **Man** as analogy *start*, **King** as analogy *end* and **Woman** as *query* text, adjust the `top_k` bar to limit the number of the returning similar items and click the `Run Interact` button to start vector search with the analogy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interact_manual(text_analogy, query=\"Woman\", start=\"Man\", end=\"King\", top_k=IntSlider(min=1, max=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0412a0b",
   "metadata": {},
   "source": [
    "Here, the returned images for the *women* query after applying the analogy are different from before in [*Search using text query*](#Search-using-text-query). An analogy can play a significant role in customizing the vector search output to the desired results.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
